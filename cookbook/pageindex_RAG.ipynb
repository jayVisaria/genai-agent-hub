{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNG3DH+mHKlBb5XdvdxQUnb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayVisaria/genai-agent-hub/blob/main/cookbook/pageindex_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebvn5qfpcG1K"
      },
      "source": [
        "# Simple Vectorless RAG with PageIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s6XXZM_-0pg"
      },
      "source": [
        "## PageIndex Introduction\n",
        "PageIndex is a new **reasoning-based**, **vectorless RAG** framework that performs retrieval in two steps:  \n",
        "1. Generate a tree structure index of documents  \n",
        "2. Perform reasoning-based retrieval through tree search  \n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://docs.pageindex.ai/images/cookbook/vectorless-rag.png\" width=\"70%\">\n",
        "</div>\n",
        "\n",
        "Compared to traditional vector-based RAG, PageIndex features:\n",
        "- **No Vectors Needed**: Uses document structure and LLM reasoning for retrieval.\n",
        "- **No Chunking Needed**: Documents are organized into natural sections rather than artificial chunks.\n",
        "- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents.\n",
        "- **Transparent Retrieval Process**: Retrieval based on reasoning â€” say goodbye to approximate semantic search (\"vibe retrieval\")."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --upgrade pageindex"
      ],
      "metadata": {
        "id": "r4uYKhZ4_Viz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pageindex import PageIndexClient\n",
        "import pageindex.utils as utils\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your PageIndex API key from https://dash.pageindex.ai/api-keys\n",
        "PAGEINDEX_API_KEY = userdata.get(\"PAGEINDEX_API_KEY\")\n",
        "pi_client = PageIndexClient(api_key=PAGEINDEX_API_KEY)"
      ],
      "metadata": {
        "id": "AkuIgZxZ_qph"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your Gemini API key from Google AI Studio\n",
        "GEMINI_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "async def call_llm(prompt, model=\"gemini-2.5-flash\", temperature=0):\n",
        "    \"\"\"Calls the Google Gemini API with the given text prompt.\"\"\"\n",
        "    client = genai.GenerativeModel(model)\n",
        "    response = client.generate_content(prompt)\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "jfT3w2FbAYBD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests\n",
        "\n",
        "# You can also use our GitHub repo to generate PageIndex tree\n",
        "# https://github.com/VectifyAI/PageIndex\n",
        "\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_path = os.path.join(\"../data\", pdf_url.split('/')[-1])\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "response = requests.get(pdf_url)\n",
        "with open(pdf_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "print(f\"Downloaded {pdf_url}\")\n",
        "\n",
        "doc_id = pi_client.submit_document(pdf_path)[\"doc_id\"]\n",
        "print('Document Submitted:', doc_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snA-SukpB_xc",
        "outputId": "c0ab9e97-f48f-4e47-d83a-d5b9ba845794"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded https://arxiv.org/pdf/2501.12948.pdf\n",
            "Document Submitted: pi-cmgc8p5gu003a0bqsc56lzrbh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heGtIMOVcG1N"
      },
      "source": [
        "## Step 1: PageIndex Tree Generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if pi_client.is_retrieval_ready(doc_id):\n",
        "    tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "    print('Simplified Tree Structure of the Document:')\n",
        "    utils.print_tree(tree)\n",
        "else:\n",
        "    print(\"Processing document, please try again later...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT58siOrCD2E",
        "outputId": "69468b2a-f612-450a-bb4b-6237ac2a3410"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified Tree Structure of the Document:\n",
            "[{'title': 'DeepSeek-R1: Incentivizing Reasoning Cap...',\n",
            "  'node_id': '0000',\n",
            "  'prefix_summary': '# DeepSeek-R1: Incentivizing Reasoning C...',\n",
            "  'nodes': [{'title': 'Abstract',\n",
            "             'node_id': '0001',\n",
            "             'summary': 'The partial document introduces two reas...'},\n",
            "            {'title': 'Contents',\n",
            "             'node_id': '0002',\n",
            "             'summary': 'This partial document outlines the struc...'},\n",
            "            {'title': '1. Introduction',\n",
            "             'node_id': '0003',\n",
            "             'prefix_summary': 'The partial document introduces recent a...',\n",
            "             'nodes': [{'title': '1.1. Contributions',\n",
            "                        'node_id': '0004',\n",
            "                        'summary': '### 1.1. Contributions\\n'},\n",
            "                       {'title': 'Post-Training: Large-Scale Reinforcement...',\n",
            "                        'node_id': '0005',\n",
            "                        'summary': 'This partial document discusses the appl...'},\n",
            "                       {'title': 'Distillation: Smaller Models Can Be Powe...',\n",
            "                        'node_id': '0006',\n",
            "                        'summary': 'This partial document discusses the effe...'},\n",
            "                       {'title': '1.2. Summary of Evaluation Results',\n",
            "                        'node_id': '0007',\n",
            "                        'summary': 'The partial document provides a summary ...'}]},\n",
            "            {'title': '2. Approach',\n",
            "             'node_id': '0008',\n",
            "             'prefix_summary': '## 2. Approach\\n',\n",
            "             'nodes': [{'title': '2.1. Overview',\n",
            "                        'node_id': '0009',\n",
            "                        'summary': '### 2.1. Overview\\n\\nPrevious work has hea...'},\n",
            "                       {'title': '2.2. DeepSeek-R1-Zero: Reinforcement Lea...',\n",
            "                        'node_id': '0010',\n",
            "                        'prefix_summary': '### 2.2. DeepSeek-R1-Zero: Reinforcement...',\n",
            "                        'nodes': [{'title': '2.2.1. Reinforcement Learning Algorithm',\n",
            "                                   'node_id': '0011',\n",
            "                                   'summary': 'The partial document describes the Group...'},\n",
            "                                  {'title': '2.2.2. Reward Modeling',\n",
            "                                   'node_id': '0012',\n",
            "                                   'summary': 'This partial document discusses the rewa...'},\n",
            "                                  {'title': '2.2.3. Training Template',\n",
            "                                   'node_id': '0013',\n",
            "                                   'summary': '#### 2.2.3. Training Template\\n\\nTo train ...'},\n",
            "                                  {'title': '2.2.4. Performance, Self-evolution Proce...',\n",
            "                                   'node_id': '0014',\n",
            "                                   'summary': 'This partial document discusses the perf...'}]},\n",
            "                       {'title': '2.3. DeepSeek-R1: Reinforcement Learning...',\n",
            "                        'node_id': '0015',\n",
            "                        'summary': 'This partial document describes the trai...'},\n",
            "                       {'title': '2.4. Distillation: Empower Small Models ...',\n",
            "                        'node_id': '0016',\n",
            "                        'summary': 'This partial document discusses the proc...'}]},\n",
            "            {'title': '3. Experiment',\n",
            "             'node_id': '0017',\n",
            "             'prefix_summary': 'The partial document describes the exper...',\n",
            "             'nodes': [{'title': '3.1. DeepSeek-R1 Evaluation',\n",
            "                        'node_id': '0018',\n",
            "                        'summary': 'This partial document presents a compreh...'},\n",
            "                       {'title': '3.2. Distilled Model Evaluation',\n",
            "                        'node_id': '0019',\n",
            "                        'summary': 'This partial document presents an evalua...'}]},\n",
            "            {'title': '4. Discussion',\n",
            "             'node_id': '0020',\n",
            "             'summary': 'This partial document discusses the comp...'},\n",
            "            {'title': '5. Conclusion, Limitations, and Future W...',\n",
            "             'node_id': '0021',\n",
            "             'summary': 'This partial document presents the concl...'},\n",
            "            {'title': 'References',\n",
            "             'node_id': '0022',\n",
            "             'summary': 'This partial document is a references se...'},\n",
            "            {'title': 'Appendix', 'node_id': '0023', 'summary': '## Appendix\\n'},\n",
            "            {'title': 'A. Contributions and Acknowledgments',\n",
            "             'node_id': '0024',\n",
            "             'summary': 'This partial document section details th...'}]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USoCLOiQcG1O"
      },
      "source": [
        "## Step 2: Reasoning-Based Retrieval with Tree Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LLHNJAtTcG1O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "query = \"What are the conclusions in this document?\"\n",
        "\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "tree_search_result = await call_llm(search_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8DVUOuAen5u",
        "outputId": "75c46a66-4b64-4efe-9448-2a02fea4c78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reasoning Process:\n",
            "The user is asking for the conclusions of the document. In academic papers, conclusions are\n",
            "typically found in the Abstract, the dedicated Conclusion section, and sometimes summarized in the\n",
            "Discussion section.\n",
            "1. **Node 0021 (5. Conclusion, Limitations, and Future Work):** This is the primary location for the\n",
            "explicit conclusions, summarizing the findings about DeepSeek-R1-Zero, DeepSeek-R1, and the\n",
            "successful distillation process.\n",
            "2. **Node 0001 (Abstract):** The abstract provides a high-level summary of the main conclusions,\n",
            "including the comparative performance of DeepSeek-R1.\n",
            "3. **Node 0020 (4. Discussion):** This section contains interpretive conclusions, specifically\n",
            "comparing the effectiveness and economy of distillation versus reinforcement learning, which is a\n",
            "major takeaway of the research.\n",
            "I will select these three nodes as they collectively provide the comprehensive set of conclusions\n",
            "and major findings.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 0001\t Page: 1\t Title: Abstract\n",
            "Node ID: 0020\t Page: 14\t Title: 4. Discussion\n",
            "Node ID: 0021\t Page: 16\t Title: 5. Conclusion, Limitations, and Future Work\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "node_map = utils.create_node_mapping(tree)\n",
        "\n",
        "tree_search_result_cleaned = tree_search_result.strip()\n",
        "if tree_search_result_cleaned.startswith(\"```json\"):\n",
        "    tree_search_result_cleaned = tree_search_result_cleaned[7:-3].strip()\n",
        "elif tree_search_result_cleaned.startswith(\"```\"):\n",
        "     tree_search_result_cleaned = tree_search_result_cleaned[3:-3].strip()\n",
        "\n",
        "\n",
        "try:\n",
        "    tree_search_result_json = json.loads(tree_search_result_cleaned)\n",
        "\n",
        "    print('Reasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json['thinking'])\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    for node_id in tree_search_result_json[\"node_list\"]:\n",
        "        node = node_map.get(node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node['page_index']}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Warning: Node ID {node_id} not found in node_map.\")\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "    print(\"Raw LLM output:\")\n",
        "    print(tree_search_result)\n",
        "except KeyError as e:\n",
        "    print(f\"Error accessing expected key in JSON: {e}\")\n",
        "    print(\"Parsed JSON object:\")\n",
        "    print(tree_search_result_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10wOZDG_cG1O"
      },
      "source": [
        "## Step 3: Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7UCBnXlcG1O",
        "outputId": "8bbb9a56-15ae-4b13-bd91-acf21a675fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved Context:\n",
            "\n",
            "## Abstract\n",
            "\n",
            "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
            "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised\n",
            "fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL,\n",
            "DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors.\n",
            "However, it encounters challenges such as poor readability, and language mixing. To address these\n",
            "issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates\n",
            "multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to\n",
            "OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source\n",
            "DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from\n",
            "DeepSeek-R1 based on Qwen and Llama.\n",
            "\n",
            "\n",
            "![img-0.jpeg](img-0.jpeg)\n",
            "\n",
            "Figure 1 | Benchmark performance of DeepSeek-R1.\n",
            "\n",
            "\n",
            "## 4. Discussion\n",
            "\n",
            "#...\n"
          ]
        }
      ],
      "source": [
        "node_list = tree_search_result_json[\"node_list\"]\n",
        "relevant_content = \"\\n\\n\".join(node_map[node_id][\"text\"] for node_id in node_list)\n",
        "\n",
        "print('Retrieved Context:\\n')\n",
        "utils.print_wrapped(relevant_content[:1000] + '...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "tcp_PhHzcG1O",
        "outputId": "41b83419-363f-4b18-fd6b-054b5f856d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            "\n",
            "The conclusions in this document are as follows:\n",
            "\n",
            "**Regarding the DeepSeek Models:**\n",
            "\n",
            "1.  **DeepSeek-R1-Zero** (the pure RL approach without cold-start data) achieved strong performance\n",
            "across various tasks, demonstrating the potential of LLMs to develop reasoning capabilities without\n",
            "supervised data. However, it encountered challenges such as poor readability and language mixing.\n",
            "2.  **DeepSeek-R1** (leveraging cold-start data alongside iterative RL fine-tuning) is more powerful\n",
            "and achieves performance comparable to OpenAI-o1-1217 on a range of tasks.\n",
            "\n",
            "**Regarding Distillation vs. Reinforcement Learning (RL):**\n",
            "\n",
            "1.  Distilling more powerful models (like DeepSeek-R1) into smaller ones yields excellent and\n",
            "effective results, with distilled models achieving impressive benchmark results, significantly\n",
            "outperforming other instruction-tuned models.\n",
            "2.  Smaller models relying on the large-scale RL discussed in the paper require enormous\n",
            "computational power and may not even achieve the performance results obtained through distillation.\n",
            "3.  While distillation strategies are economical and effective, advancing beyond current boundaries\n",
            "of intelligence may still require more powerful base models and larger-scale reinforcement learning.\n",
            "\n",
            "**Regarding Unsuccessful Attempts (PRM and MCTS):**\n",
            "\n",
            "1.  The advantages of the **Process Reward Model (PRM)** are limited compared to the additional\n",
            "computational overhead it introduces during large-scale reinforcement learning.\n",
            "2.  Iteratively boosting model performance through self-search methods like **Monte Carlo Tree\n",
            "Search (MCTS)** remains a significant challenge due to the complexities of token generation and the\n",
            "difficulty of training a fine-grained value model.\n"
          ]
        }
      ],
      "source": [
        "answer_prompt = f\"\"\"\n",
        "Answer the question based on the context:\n",
        "\n",
        "Question: {query}\n",
        "Context: {relevant_content}\n",
        "\n",
        "Provide a clear, concise answer based only on the context provided.\n",
        "\"\"\"\n",
        "\n",
        "print('Generated Answer:\\n')\n",
        "answer = await call_llm(answer_prompt)\n",
        "utils.print_wrapped(answer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21c241ab"
      },
      "source": [
        "# Define the user query\n",
        "query = \"What are Distilled Model Evaluation?\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e0220443",
        "outputId": "ed23a792-1485-4457-9221-4bd09fb1559b"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "import asyncio\n",
        "\n",
        "from pageindex import PageIndexClient\n",
        "import pageindex.utils as utils\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "# --- Step 1: Setup and Initialization ---\n",
        "# Get your API keys from Google Colab secrets\n",
        "PAGEINDEX_API_KEY = userdata.get(\"PAGEINDEX_API_KEY\")\n",
        "GEMINI_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Initialize PageIndex client\n",
        "pi_client = PageIndexClient(api_key=PAGEINDEX_API_KEY)\n",
        "\n",
        "# Initialize Gemini client\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "async def call_llm(prompt, model=\"gemini-2.5-flash\", temperature=0):\n",
        "    \"\"\"Calls the Google Gemini API with the given text prompt.\"\"\"\n",
        "    client = genai.GenerativeModel(model)\n",
        "    response = client.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "\n",
        "# --- Step 2: Document Loading and Indexing ---\n",
        "# Download the document\n",
        "pdf_url = \"https://arxiv.org/pdf/2501.12948.pdf\"\n",
        "pdf_path = os.path.join(\"../data\", pdf_url.split('/')[-1])\n",
        "os.makedirs(os.path.dirname(pdf_path), exist_ok=True)\n",
        "\n",
        "response = requests.get(pdf_url)\n",
        "with open(pdf_path, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "print(f\"Downloaded {pdf_url}\")\n",
        "\n",
        "# Submit document to PageIndex for indexing\n",
        "doc_id = pi_client.submit_document(pdf_path)[\"doc_id\"]\n",
        "print('Document Submitted:', doc_id)\n",
        "\n",
        "# Wait for document to be ready (optional, but good practice)\n",
        "while not pi_client.is_retrieval_ready(doc_id):\n",
        "    print(\"Processing document, please wait...\")\n",
        "    await asyncio.sleep(5) # Use asyncio.sleep in async function\n",
        "\n",
        "# --- Step 3: PageIndex Tree Retrieval ---\n",
        "tree = pi_client.get_tree(doc_id, node_summary=True)['result']\n",
        "node_map = utils.create_node_mapping(tree)\n",
        "print('Simplified Tree Structure of the Document:')\n",
        "utils.print_tree(tree)\n",
        "\n",
        "\n",
        "# --- RAG Pipeline from Query to Answer ---\n",
        "\n",
        "# The query is defined in a cell above (assuming 'query' variable exists)\n",
        "# query = \"What are the conclusions in this document?\" # Uncomment and define query here if running as a standalone cell\n",
        "\n",
        "# Step 4: Reasoning-Based Tree Search\n",
        "# Use the LLM to find relevant nodes in the document tree\n",
        "tree_without_text = utils.remove_fields(tree.copy(), fields=['text'])\n",
        "\n",
        "search_prompt = f\"\"\"\n",
        "You are given a question and a tree structure of a document.\n",
        "Each node contains a node id, node title, and a corresponding summary.\n",
        "Your task is to find all nodes that are likely to contain the answer to the question.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Document tree structure:\n",
        "{json.dumps(tree_without_text, indent=2)}\n",
        "\n",
        "Please reply in the following JSON format:\n",
        "{{\n",
        "    \"thinking\": \"<Your thinking process on which nodes are relevant to the question>\",\n",
        "    \"node_list\": [\"node_id_1\", \"node_id_2\", ..., \"node_id_n\"]\n",
        "}}\n",
        "Directly return the final JSON structure. Do not output anything else.\n",
        "\"\"\"\n",
        "\n",
        "tree_search_result = await call_llm(search_prompt)\n",
        "\n",
        "# Attempt to clean and parse the LLM output\n",
        "tree_search_result_cleaned = tree_search_result.strip()\n",
        "if tree_search_result_cleaned.startswith(\"```json\"):\n",
        "    tree_search_result_cleaned = tree_search_result_cleaned[7:-3].strip()\n",
        "elif tree_search_result_cleaned.startswith(\"```\"):\n",
        "     tree_search_result_cleaned = tree_search_result_cleaned[3:-3].strip()\n",
        "\n",
        "try:\n",
        "    tree_search_result_json = json.loads(tree_search_result_cleaned)\n",
        "\n",
        "    print('\\nReasoning Process:')\n",
        "    utils.print_wrapped(tree_search_result_json['thinking'])\n",
        "\n",
        "    print('\\nRetrieved Nodes:')\n",
        "    for node_id in tree_search_result_json[\"node_list\"]:\n",
        "        node = node_map.get(node_id)\n",
        "        if node:\n",
        "            print(f\"Node ID: {node['node_id']}\\t Page: {node['page_index']}\\t Title: {node['title']}\")\n",
        "        else:\n",
        "            print(f\"Warning: Node ID {node_id} not found in node_map.\")\n",
        "\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "    print(\"Raw LLM output:\")\n",
        "    print(tree_search_result)\n",
        "    tree_search_result_json = {}\n",
        "except KeyError as e:\n",
        "    print(f\"Error accessing expected key in JSON: {e}\")\n",
        "    print(\"Parsed JSON object:\")\n",
        "    print(tree_search_result_json)\n",
        "    tree_search_result_json = {}\n",
        "\n",
        "\n",
        "# Step 5: Retrieve Relevant Content\n",
        "if 'node_list' in tree_search_result_json:\n",
        "    node_list = tree_search_result_json[\"node_list\"]\n",
        "    relevant_content = \"\\n\\n\".join(node_map[node_id][\"text\"] for node_id in node_list if node_id in node_map)\n",
        "\n",
        "    print('\\nRetrieved Context:\\n')\n",
        "    utils.print_wrapped(relevant_content[:1000] + '...')\n",
        "else:\n",
        "    relevant_content = \"\"\n",
        "    print(\"\\nNo nodes were identified as relevant.\")\n",
        "\n",
        "\n",
        "# Step 6: Answer Generation\n",
        "if relevant_content:\n",
        "    answer_prompt = f\"\"\"\n",
        "    Answer the question based on the context:\n",
        "\n",
        "    Question: {query}\n",
        "    Context: {relevant_content}\n",
        "\n",
        "    Provide a clear, concise answer based only on the context provided.\n",
        "    \"\"\"\n",
        "\n",
        "    print('\\nGenerated Answer:\\n')\n",
        "    answer = await call_llm(answer_prompt)\n",
        "    utils.print_wrapped(answer)\n",
        "else:\n",
        "    print(\"\\nCannot generate answer as no relevant content was retrieved.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded https://arxiv.org/pdf/2501.12948.pdf\n",
            "Document Submitted: pi-cmgc8xv51016p0aqs01ch8ey6\n",
            "Processing document, please wait...\n",
            "Processing document, please wait...\n",
            "Processing document, please wait...\n",
            "Processing document, please wait...\n",
            "Processing document, please wait...\n",
            "Processing document, please wait...\n",
            "Simplified Tree Structure of the Document:\n",
            "[{'title': 'DeepSeek-R1: Incentivizing Reasoning Cap...',\n",
            "  'node_id': '0000',\n",
            "  'prefix_summary': '# DeepSeek-R1: Incentivizing Reasoning C...',\n",
            "  'nodes': [{'title': 'Abstract',\n",
            "             'node_id': '0001',\n",
            "             'summary': 'The partial document introduces two reas...'},\n",
            "            {'title': 'Contents',\n",
            "             'node_id': '0002',\n",
            "             'summary': 'The partial document outlines the struct...'},\n",
            "            {'title': '1. Introduction',\n",
            "             'node_id': '0003',\n",
            "             'prefix_summary': 'The partial document introduces recent a...',\n",
            "             'nodes': [{'title': '1.1. Contributions',\n",
            "                        'node_id': '0004',\n",
            "                        'summary': '### 1.1. Contributions\\n'},\n",
            "                       {'title': 'Post-Training: Large-Scale Reinforcement...',\n",
            "                        'node_id': '0005',\n",
            "                        'summary': 'This partial document discusses the appl...'},\n",
            "                       {'title': 'Distillation: Smaller Models Can Be Powe...',\n",
            "                        'node_id': '0006',\n",
            "                        'summary': 'This partial document discusses the effe...'},\n",
            "                       {'title': '1.2. Summary of Evaluation Results',\n",
            "                        'node_id': '0007',\n",
            "                        'summary': 'The partial document provides a summary ...'}]},\n",
            "            {'title': '2. Approach',\n",
            "             'node_id': '0008',\n",
            "             'prefix_summary': '## 2. Approach\\n',\n",
            "             'nodes': [{'title': '2.1. Overview',\n",
            "                        'node_id': '0009',\n",
            "                        'summary': '### 2.1. Overview\\n\\nPrevious work has hea...'},\n",
            "                       {'title': '2.2. DeepSeek-R1-Zero: Reinforcement Lea...',\n",
            "                        'node_id': '0010',\n",
            "                        'prefix_summary': '### 2.2. DeepSeek-R1-Zero: Reinforcement...',\n",
            "                        'nodes': [{'title': '2.2.1. Reinforcement Learning Algorithm',\n",
            "                                   'node_id': '0011',\n",
            "                                   'summary': 'The partial document describes the Group...'},\n",
            "                                  {'title': '2.2.2. Reward Modeling',\n",
            "                                   'node_id': '0012',\n",
            "                                   'summary': 'This partial document discusses the rewa...'},\n",
            "                                  {'title': '2.2.3. Training Template',\n",
            "                                   'node_id': '0013',\n",
            "                                   'summary': '#### 2.2.3. Training Template\\n\\nTo train ...'},\n",
            "                                  {'title': '2.2.4. Performance, Self-evolution Proce...',\n",
            "                                   'node_id': '0014',\n",
            "                                   'summary': 'This partial document discusses the perf...'}]},\n",
            "                       {'title': '2.3. DeepSeek-R1: Reinforcement Learning...',\n",
            "                        'node_id': '0015',\n",
            "                        'summary': 'This partial document describes the trai...'},\n",
            "                       {'title': '2.4. Distillation: Empower Small Models ...',\n",
            "                        'node_id': '0016',\n",
            "                        'summary': 'This partial document discusses the proc...'}]},\n",
            "            {'title': '3. Experiment',\n",
            "             'node_id': '0017',\n",
            "             'prefix_summary': 'The partial document describes the exper...',\n",
            "             'nodes': [{'title': '3.1. DeepSeek-R1 Evaluation',\n",
            "                        'node_id': '0018',\n",
            "                        'summary': 'This partial document presents a compreh...'},\n",
            "                       {'title': '3.2. Distilled Model Evaluation',\n",
            "                        'node_id': '0019',\n",
            "                        'summary': 'This partial document section presents a...'}]},\n",
            "            {'title': '4. Discussion',\n",
            "             'node_id': '0020',\n",
            "             'summary': 'This partial document discusses the comp...'},\n",
            "            {'title': '5. Conclusion, Limitations, and Future W...',\n",
            "             'node_id': '0021',\n",
            "             'summary': 'This partial document presents the concl...'},\n",
            "            {'title': 'References',\n",
            "             'node_id': '0022',\n",
            "             'summary': 'This partial document is a references se...'},\n",
            "            {'title': 'Appendix', 'node_id': '0023', 'summary': '## Appendix\\n'},\n",
            "            {'title': 'A. Contributions and Acknowledgments',\n",
            "             'node_id': '0024',\n",
            "             'summary': 'This partial document section details th...'}]}]\n",
            "\n",
            "Reasoning Process:\n",
            "The user is asking about 'Distilled Model Evaluation'. I need to find all nodes that specifically\n",
            "talk about the evaluation, performance, or results of distilled models. I will look for keywords\n",
            "like 'distilled model', 'evaluation', 'experiment', 'performance', 'benchmark', 'results'.\n",
            "\n",
            "1.  **Abstract (0001)**: Mentions 'six distilled dense models' and a 'benchmark performance figure'.\n",
            "This is a high-level summary, relevant for an initial understanding.\n",
            "2.  **Contents (0002)**: Explicitly states 'The experimental section covers evaluations of both the\n",
            "main and distilled models.' This is a navigational clue pointing to the relevant section.\n",
            "3.  **1. Introduction (0003)**: Introduces the concept of 'distillation of reasoning abilities into\n",
            "smaller models' and states that 'distilled models outperform previous state-of-the-art'. This\n",
            "provides an early answer to the 'what' and 'why' of their evaluation.\n",
            "4.  **Distillation: Smaller Models Can Be Powerful Too (0006)**: Discusses the effectiveness of\n",
            "distillation and mentions 'distilled models that achieve strong benchmark results'. This reinforces\n",
            "the idea that these models are evaluated.\n",
            "5.  **2.4. Distillation: Empower Small Models with Reasoning Capability (0016)**: While primarily\n",
            "about the *approach* to distillation, its summary states that the process 'significantly improves\n",
            "their reasoning abilities', which is a direct outcome of evaluation.\n",
            "6.  **3. Experiment (0017)**: This is the overarching section for all evaluations. Its summary sets\n",
            "the context for how models (including distilled ones) are evaluated across benchmarks.\n",
            "7.  **3.2. Distilled Model Evaluation (0019)**: This node's title is a perfect match for the query.\n",
            "Its summary directly describes the 'evaluation of various distilled language models' and their\n",
            "'comparative performance table' on benchmarks. This is the most critical node.\n",
            "8.  **4. Discussion (0020)**: Discusses the 'comparative effectiveness of distillation versus\n",
            "reinforcement learning', specifically mentioning 'experimental results showing that distilling a\n",
            "powerful model... yields significantly better performance'. This analyzes the findings from the\n",
            "evaluation.\n",
            "9.  **5. Conclusion, Limitations, and Future Work (0021)**: Summarizes the 'successful distillation\n",
            "of reasoning capabilities into smaller dense models' and highlights specific performance metrics\n",
            "(e.g., 'outperforming leading models like GPT-4o and Claude-3.5-Sonnet on math benchmarks'). This\n",
            "provides a conclusive overview of their evaluated performance.\n",
            "\n",
            "All these nodes contribute to understanding what Distilled Model Evaluation entails, from its\n",
            "introduction and methodology to its results and implications.\n",
            "\n",
            "Retrieved Nodes:\n",
            "Node ID: 0001\t Page: 1\t Title: Abstract\n",
            "Node ID: 0002\t Page: 2\t Title: Contents\n",
            "Node ID: 0003\t Page: 3\t Title: 1. Introduction\n",
            "Node ID: 0006\t Page: 4\t Title: Distillation: Smaller Models Can Be Powerful Too\n",
            "Node ID: 0016\t Page: 11\t Title: 2.4. Distillation: Empower Small Models with Reasoning Capability\n",
            "Node ID: 0017\t Page: 11\t Title: 3. Experiment\n",
            "Node ID: 0019\t Page: 14\t Title: 3.2. Distilled Model Evaluation\n",
            "Node ID: 0020\t Page: 14\t Title: 4. Discussion\n",
            "Node ID: 0021\t Page: 16\t Title: 5. Conclusion, Limitations, and Future Work\n",
            "\n",
            "Retrieved Context:\n",
            "\n",
            "## Abstract\n",
            "\n",
            "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
            "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised\n",
            "fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL,\n",
            "DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors.\n",
            "However, it encounters challenges such as poor readability, and language mixing. To address these\n",
            "issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates\n",
            "multi-stage training and cold-start data before RL. DeepSeekR1 achieves performance comparable to\n",
            "OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source\n",
            "DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from\n",
            "DeepSeek-R1 based on Qwen and Llama.\n",
            "\n",
            "\n",
            "![img-0.jpeg](img-0.jpeg)\n",
            "\n",
            "Figure 1 | Benchmark performance of DeepSeek-R1.\n",
            "\n",
            "\n",
            "## Contents\n",
            "\n",
            "1 Intr...\n",
            "\n",
            "Generated Answer:\n",
            "\n",
            "Distilled Model Evaluation refers to the assessment of smaller dense models (such as Qwen2.5 and\n",
            "Llama3 series, including 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints) that have been fine-tuned\n",
            "using 800k reasoning samples curated with the DeepSeek-R1 model.\n",
            "\n",
            "These distilled models are evaluated on reasoning-related benchmarks, specifically AIME 2024,\n",
            "MATH-500, GPQA Diamond, LiveCodeBench, and Codeforces. The evaluation uses metrics such as pass@1,\n",
            "cons@64 (majority vote), and rating to demonstrate the effectiveness of distilling reasoning\n",
            "capabilities from a larger, more powerful model (DeepSeek-R1) into smaller, more efficient models.\n"
          ]
        }
      ]
    }
  ]
}